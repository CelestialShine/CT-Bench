# ğŸ§  CT-Bench: Benchmarking Multimodal AI in Computed Tomography

This repository contains the official code for our NeurIPS 2025 Datasets & Benchmarks (DB) Track submission:

**CT-Bench: A Comprehensive Benchmark for Multimodal AI in Computed Tomography Analysis**

CT-Bench supports multimodal lesion-level CT analysis with:
- ğŸ§© 20,335 annotated lesions (bounding boxes, size, descriptions)
- ğŸ§  2,850 expert-verified QA tasks spanning 7 clinical reasoning tasks
- âš™ï¸ Baselines with GPT-4V, BiomedCLIP, and fine-tuned models

---

## ğŸ“¦ Dataset

The dataset includes:
- **Lesion Image & Metadata Set**: lesion slices (with/without BBox), metadata (size, description)
- **QA Benchmark Component**: VQA tasks in both LLM and CLIP styles, with hard negatives

ğŸ“ **Download instructions and access links are included in the NeurIPS submission.**

### Folder Structure After Extraction

```
data/
â”œâ”€â”€ clip/              # QA pairs for CLIP-style models
â”œâ”€â”€ llm/               # QA pairs for LLM-style models
â”œâ”€â”€ lesion_bbox/       # PNG images with bounding boxes (cropped)
â”œâ”€â”€ lesion_nobox/      # PNG images without bounding boxes
â”œâ”€â”€ Metadata.csv       # Lesion info: id, description, size, split
â”œâ”€â”€ qa_clip.json       # Structured QA for CLIP models
â”œâ”€â”€ qa_llm.json        # Structured QA for LLM models
```

---

## ğŸ—‚ï¸ Project Structure

```
ct-bench/
â”œâ”€â”€ captioning/                    # Image captioning with GPT-4V or others
â”‚   â””â”€â”€ image_caption.py
â”œâ”€â”€ training/                      # Fine-tuning BiomedCLIP, RadFM, etc.
â”‚   â””â”€â”€ biomedclip.py
â”œâ”€â”€ qa_llm/                        # QA evaluation using LLM-style models
â”‚   â””â”€â”€ run_qa_vlm.py
â”œâ”€â”€ qa_clip/                       # QA evaluation using CLIP-style models
â”‚   â””â”€â”€ run_qa_clip.py
â”œâ”€â”€ models/                        # Modular model wrappers
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py
â”‚   â”œâ”€â”€ gpt4v.py
â”‚   â”œâ”€â”€ biomedclip.py
â”œâ”€â”€ data/                          # External dataset (see above)
â”œâ”€â”€ results/                       # Output logs, captions, predictions
â”œâ”€â”€ configs/                       # Optional config files
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ–¼ï¸ 1. Lesion Image Captioning with GPT-4V

Generate CT lesion descriptions using GPT-4V.

```bash
python captioning/image_caption.py \
  --image_dir data/lesion_nobox \
  --metadata data/Metadata.csv \
  --output_dir results/captions
```

- Input: `.png` images, `Metadata.csv`
- Output: `gpt4v_captions.json` with model-generated and ground-truth captions
- Requires `OPENAI_API_KEY` to be set in your environment

---

## ğŸ§ª 2. Fine-Tuning BiomedCLIP on CT-Bench

Train contrastive models using bounding box lesion slices.

```bash
python training/biomedclip.py \
  --train --input_dir data/lesion_bbox \
  --metadata data/Metadata.csv \
  --output_dir results/biomedclip
```

Supports both with and without BBox supervision:
- Use `--with_bbox` flag to include cropped regions
- Logs and model checkpoints saved to `results/biomedclip/`

---

## ğŸ¤– 3. QA Benchmark: LLM-Style (GPT-4V, Gemini, etc.)

Evaluate large language-vision models on clinical lesion QA tasks.

```bash
python qa_llm/run_qa_vlm.py \
  --model gpt4v \
  --qa_file data/qa_llm.json \
  --input_dir data/llm/test
```

Supported tasks:
- `img2txt`, `ct2txt`, `img2attrib`, `ct2attrib`, `img2size`, etc.

---

## ğŸ¯ 4. QA Benchmark: CLIP-Style (BiomedCLIP, PMC-CLIP)

Use image-text retrieval to answer QA tasks with hard negatives.

```bash
python qa_clip/run_qa_clip.py \
  --model biomedclip \
  --qa_file data/qa_clip.json \
  --input_dir data/clip/test
```

Tasks include:
- `txt2img`, `txt2bbox`, `img2attrib`

---

## âš™ï¸ Requirements

Install dependencies with:

```bash
pip install -r requirements.txt
```

Includes:
- Python 3.8+
- PyTorch 2.0+
- OpenAI SDK
- Pillow, TQDM, etc.

---

## ğŸ”’ Reproducibility & Anonymity

- This repository is anonymized for NeurIPS review.
- Dataset access and additional models (e.g., Gemini, Dragonfly) are described in the paper.
- Only GPT-4V and BiomedCLIP are exposed for simplicity and reproducibility.

---

## ğŸ“¬ Contact

Please refer to the NeurIPS submission for correspondence and dataset request info.
